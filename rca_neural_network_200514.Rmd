---
title: "MLM Term Paper R Code"
author: "Wooyong Jung (wj710)"
date: "5/14/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, tidy=TRUE, message = FALSE, warning = FALSE)
```

```{r basic setting}
# Load libraries
library(haven)
library(ggplot2)
library(tidyverse)
library(keras)
library(tfruns)
```

```{r data cleansing for RCA}
# Load dataset
## RCA data
load("Termpaper/dataset/country_sitcproduct4digit_year.RData")
## SITC product and country(location) code
pid <- read_dta("Termpaper/dataset/sitc_product.dta")
loc <- read_dta("Termpaper/dataset/location.dta")

# Merge pid and loc into main data frame
df <- merge(table, pid[,-5], by="product_id", all.x = TRUE)
df <- merge(df, loc[,c(1,2,3,4)], by="location_id", all.x = TRUE)

# Select variables needed for this study
df <- df %>% 
  select(location_id, product_id, year, export_rca, sitc_product_name_short_en, location_code.x, location_name_short_en)

# Change column name
colnames(df) <- c("lid", "pid", "year", "rca", "product", "iso", "country")
```

```{r data cleansing for GDP growth}
# Load gdp growth
gdp <- read_csv("Termpaper/dataset/gdp_growth.csv")

# Convert to long data
gdp <- gather(gdp, year, growth, `1961`:`2018`)

# Check quantile level
quantile(gdp$growth, na.rm = TRUE)

# Create growth level variable (1 to 4)
gdp %>% 
  mutate(growth = ntile(growth, 4)) -> gdp

# Convert growth level 0 to 3
gdp <- gdp %>% 
  mutate(growth = ifelse(growth == 1, 0,
                           ifelse(growth == 2, 1,
                                  ifelse(growth == 3, 2,
                                         ifelse(growth == 4, 3, NA)))))

```

```{r data cleansing for TFP}
# Load Penn World Table data
pwt <- read_csv("Termpaper/dataset/penntable.csv")

# Choose variables needed
pwt <- pwt %>% 
  select(iso, country, year, ctfp)

# Convert to long data
pwt %>% 
  mutate(tfp = ntile(ctfp, 5)) -> pwt

# Check quantile level
quantile(pwt$ctfp, na.rm = TRUE)
pwt <- pwt[,-4]

# Convert TFP level 0 to 3
pwt <- pwt %>% 
  mutate(tfp = ifelse(tfp == 1, 0,
                           ifelse(tfp == 2, 1,
                                  ifelse(tfp == 3, 2,
                                         ifelse(tfp == 4, 3, NA)))))
```

```{r merge RCA, growth and TFP datasets}
# Merge growth into df and make new dataframe (df.growth)
df_growth <- merge(df, gdp, by = c("iso", "year"), all.x = TRUE)

# Remain only complete cases and select variables needed
df_growth <- df_growth %>% 
  drop_na() %>%
  select(iso, year, pid, rca, growth)

# Convert to wide format (products in column) and Change NA to 0
## NA means that the country has no RCA of the product
df_growth <- df_growth %>% 
  spread(pid, rca) %>% 
  mutate_at(vars(3:785), funs(ifelse(is.na(.), 0, .)))

# Convert RCA as a binary
df_growth <- df_growth %>% 
  mutate_at(vars(4:785), funs(ifelse(.>1, 1, 0)))

# Merge TFP into df and make new dataframe (df_tfp)
df_tfp <- merge(df, pwt, by = c("iso", "year"), all.x = TRUE)

# Remain only complete cases and select variables needed
df_tfp <- df_tfp %>% 
  drop_na() %>% 
  select(iso, year, pid, rca, tfp)

# Convert  to wide (products in column) and Change NA to 0
## NA means that the country has no RCA of the product
df_tfp <- df_tfp %>% 
  spread(pid, rca) %>% 
  mutate_at(vars(3:785), funs(ifelse(is.na(.), 0, .)))

# Convert RCA as a binary
df_tfp <- df_tfp %>% 
  mutate_at(vars(4:785), funs(ifelse(.>1, 1, 0)))
```

```{r MLP for predicting growth rate}
# Convert df.growth into a matrix
df_growth <- df_growth[,-c(1,2)]
mat_growth <- as.matrix(df_growth)
dimnames(mat_growth) <- NULL

# Seperate into train and test set
set.seed(123)
idx_growth <- sample(2, nrow(mat_growth), replace = TRUE, prob = c(0.8, 0.2))
growth_x_train <- mat_growth[idx_growth == 1, 2:783]
growth_x_test <- mat_growth[idx_growth == 2, 2:783]

growth_y_test_actual <- mat_growth[idx_growth == 2, 1]

growth_y_train <- to_categorical(mat_growth[idx_growth == 1, 1])
growth_y_test <- to_categorical(mat_growth[idx_growth == 2, 1])

cbind(growth_y_test_actual[1:10], growth_y_test[1:10,])
```

```{r Baseline model}
# Hyperparameter tuning
runs <- tuning_run("tuning1.R",
                   flags = list(dense_units1 = c(32, 64),
                                dense_units2 = c(32, 64),
                                dense_units3 = c(16, 32),
                                activation1 = c("relu", "sigmoid"),
                                activation2 = c("relu", "sigmoid"),
                                activation3 = c("relu", "sigmoid")))

# Create a baseline model
bm_growth <- keras_model_sequential()
bm_growth %>% 
  layer_dense(units = 64, activation = "relu", input_shape = 782) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 20, activation = "relu") %>% 
  layer_dense(units = 4, activation = "softmax") 
summary(bm_growth)

# Compile the model
bm_growth %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "adam",
          metrics = c("accuracy"))

# Fit the data
history <- bm_growth %>% 
  fit(growth_x_train, growth_y_train, epoch = 10, batch_size = 64, validation_split = 0.1, verbose = 2)

```


```{r L2 Regularization}
# Create a baseline model
l2_growth <- keras_model_sequential()
l2_growth %>% 
  layer_dense(units = 50, activation = "relu", input_shape = 782,
              kernel_regularizer = regularizer_l2(0.01)) %>% 
  layer_dense(units = 20, activation = "relu",
              kernel_regularizer = regularizer_l2(0.01)) %>% 
  layer_dense(units = 20, activation = "relu",
              kernel_regularizer = regularizer_l2(0.01)) %>% 
  layer_dense(units = 4, activation = "softmax") 
summary(l2_growth)

# Compile the model
l2_growth %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "adam",
          metrics = c("accuracy")) 

# Fit the data
history <- l2_growth %>% 
  fit(growth_x_train, growth_y_train, epoch = 10, batch_size = 64, validation_split = 0.1, verbose = 2)

```

```{r Dropout}
# Hyperparameter tuning
runs <- tuning_run("experiment.R",
                   flags = list(dense_units1 = c(32, 64),
                                dense_units2 = c(32, 64),
                                dropout1 = c(0.4, 0.6),
                                dropout2 = c(0.4, 0.6)))


# Create a dropout model
dp_growth <- keras_model_sequential()
dp_growth %>% 
  layer_dense(units = 64, activation = "relu", input_shape = 782) %>% 
  layer_dropout(0.5) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(0.5) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dropout(0.5) %>% 
  layer_dense(units = 4, activation = "softmax") 
summary(dp_growth)

# Compile the model
dp_growth %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "adam",
          metrics = c("accuracy"))

# Fit the data
history <- dp_growth %>% 
  fit(growth_x_train, growth_y_train, epoch = 10, batch_size = 64, validation_split = 0.1, verbose = 2)

```



# Seperate into train and test set
set.seed(123)
idx <- sample(2, nrow(df.mat), replace = TRUE, prob = c(0.9, 0.1))
head(df3)
x_train <- df.mat[idx == 1, 4:785]
x_test <- df.mat[idx == 2, 4:785]

y_test_actual <- df.mat[idx == 2, 3]

y_train <- to_categorical(df.mat[idx == 1, 3])
y_test <- to_categorical(df.mat[idx == 2, 3])

cbind(y_test_actual[1:10], y_test[1:10,])


# Create a model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 20, activation = "relu", input_shape = 782) %>% 
  layer_dense(units = 20, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax") 
summary(model)

# Compile the model
model %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "rmsprop",
          metrics = c("accuracy"))

# Fit the data
history <- model %>% 
  fit(x_train, y_train, epoch = 10, batch_size = 30, validation_split = 0.1, verbose = 2)



###########################
# Transformation into a matrix
df.mat <- as.matrix(df5)
dimnames(df.mat) = NULL


############
# Binary
head(df5)
df6 <- df5 %>% 
  mutate_at(vars(2:783), funs(ifelse(.>=1, 1, 0)))
head(df6)

# Transformation into a matrix
df.mat <- as.matrix(df6)
dimnames(df.mat) = NULL

# Seperate into train and test set
set.seed(123)
idx <- sample(2, nrow(df.mat), replace = TRUE, prob = c(0.9, 0.1))
head(df6)
x_train <- df.mat[idx == 1, 2:783]
x_test <- df.mat[idx == 2, 2:783]

y_test_actual <- df.mat[idx == 2, 1]

y_train <- to_categorical(df.mat[idx == 1, 1])
y_test <- to_categorical(df.mat[idx == 2, 1])

cbind(y_test_actual[1:10], y_test[1:10,])


# Create a model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 70, activation = "relu", input_shape = 782) %>% 
  layer_dense(units = 40, activation = "relu") %>% 
  layer_dense(units = 20, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax") 
summary(model)

# Compile the model
model %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "rmsprop",
          metrics = c("accuracy"))

# Fit the data
history <- model %>% 
  fit(x_train, y_train, epoch = 10, batch_size = 30, verbose = 2)

metrics <- model %>% 
  evaluate(x_test, y_test)
metrics

model %>% predict_classes(x_test[1:10,])
y_test_actual[1:10]
length(unique(table$product_id))

unique(df3$year)
length(unique(df2$pid))

head(df2)
df2 %>% 
  group_by(year) %>% 
  summarise(nc = count(iso))

################################################


str(df_tfp2)

df_tfp3 <- df_tfp2[,-c(1,2)]

# Transformation into a matrix
mat.tfp <- as.matrix(df_tfp3)
dimnames(mat.tfp) = NULL

# Seperate into train and test set
set.seed(123)
idx <- sample(2, nrow(mat.tfp), replace = TRUE, prob = c(0.8, 0.2))
head(mat.tfp)
tfp_x_train <- mat.tfp[idx == 1, 2:783]
tfp_x_test <- mat.tfp[idx == 2, 2:783]

tfp_y_test_actual <- mat.tfp[idx == 2, 1]

tfp_y_train <- to_categorical(mat.tfp[idx == 1, 1])
tfp_y_test <- to_categorical(mat.tfp[idx == 2, 1])

cbind(tfp_y_test_actual[1:10], tfp_y_test[1:10,])


# Create a model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 50, activation = "relu", input_shape = 782) %>% 
  layer_dense(units = 30, activation = "relu") %>% 
  layer_dense(units = 20, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax") 
summary(model)

# Compile the model
model %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "adam",
          metrics = c("accuracy"))

# Fit the data
history <- model %>% 
  fit(tfp_x_train, tfp_y_train, epoch = 10, batch_size = 64, validation_split = 0.1, verbose = 2)




################################################
# TFP

pwt <- read_csv("Termpaper/dataset/penntable.csv")
head(pwt)
pwt <- pwt %>% 
  select(iso, country, year, ctfp)


# Transform to long data
head(pwt)
pwt %>% 
  mutate(tfp = ntile(ctfp, 5)) -> pwt

pwt <- pwt[,-4]

# Merge TFP into df
head(df)
df_tfp <- merge(df, pwt, by = c("iso", "year"), all.x = TRUE)
unique(df$iso)
unique(pwt$iso)

head(df_tfp)

# Remain only complete cases
df_tfp <- df_tfp %>% 
  drop_na()

head(df_tfp)
df_tfp <- df_tfp %>% 
  select(iso, year, pid, rca, tfp)

# Transform to wide
df_tfp <- df_tfp %>% 
  spread(pid, rca)

head(df_tfp)

df_tfp2 <- df_tfp %>% 
  mutate_at(vars(3:785), funs(ifelse(is.na(.), 0, .)))

df_tfp2 <- df_tfp2 %>% 
  mutate(tfp = ifelse(tfp == 1, 0,
                      ifelse(tfp == 2, 1,
                             ifelse(tfp == 3, 2,
                                    ifelse(tfp == 4, 3, 4)))))

# Binary
head(df_tfp2)
df_tfp2 <- df_tfp2 %>% 
  mutate_at(vars(4:785), funs(ifelse(.>=1, 1, 0)))
str(df_tfp2)

df_tfp3 <- df_tfp2[,-c(1,2)]

# Transformation into a matrix
mat.tfp <- as.matrix(df_tfp3)
dimnames(mat.tfp) = NULL

# Seperate into train and test set
set.seed(123)
idx <- sample(2, nrow(mat.tfp), replace = TRUE, prob = c(0.8, 0.2))
head(mat.tfp)
tfp_x_train <- mat.tfp[idx == 1, 2:783]
tfp_x_test <- mat.tfp[idx == 2, 2:783]

tfp_y_test_actual <- mat.tfp[idx == 2, 1]

tfp_y_train <- to_categorical(mat.tfp[idx == 1, 1])
tfp_y_test <- to_categorical(mat.tfp[idx == 2, 1])

cbind(tfp_y_test_actual[1:10], tfp_y_test[1:10,])


# Create a model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 50, activation = "relu", input_shape = 782) %>% 
  layer_dense(units = 30, activation = "relu") %>% 
  layer_dense(units = 20, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax") 
summary(model)

# Compile the model
model %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "adam",
          metrics = c("accuracy"))

# Fit the data
history <- model %>% 
  fit(tfp_x_train, tfp_y_train, epoch = 10, batch_size = 64, validation_split = 0.1, verbose = 2)


# smaller model
# Create a model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 10, activation = "relu", input_shape = 782) %>% 
  layer_dense(units = 10, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax") 
summary(model)

# Compile the model
model %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "adam",
          metrics = c("accuracy"))

# Fit the data
history <- model %>% 
  fit(tfp_x_train, tfp_y_train, epoch = 10, batch_size = 64, validation_split = 0.1, verbose = 2)


# L2 Regularization
# Create a model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 50, activation = "relu", input_shape = 782,
              kernel_regularizer = regularizer_l2(0.1)) %>% 
  layer_dense(units = 30, activation = "relu",
              kernel_regularizer = regularizer_l2(0.1)) %>% 
  layer_dense(units = 20, activation = "relu",
              kernel_regularizer = regularizer_l2(0.1)) %>% 
  layer_dense(units = 5, activation = "softmax") 
summary(model)

# Compile the model 
model %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "adam",
          metrics = c("accuracy"))

# Fit the data
history <- model %>% 
  fit(tfp_x_train, tfp_y_train, epoch = 10, batch_size = 64, validation_split = 0.1, verbose = 2)



# Dropout
# Create a model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 50, activation = "relu", input_shape = 782) %>% 
  layer_dropout(0.6) %>% 
  layer_dense(units = 30, activation = "relu") %>% 
  layer_dropout(0.6) %>% 
  layer_dense(units = 20, activation = "relu") %>% 
  layer_dropout(0.6) %>% 
  layer_dense(units = 5, activation = "softmax") 
summary(model)

# Compile the model 
model %>% 
  compile(loss = "categorical_crossentropy",
          optimizer = "adam",
          metrics = c("accuracy"))

# Fit the data
history <- model %>% 
  fit(tfp_x_train, tfp_y_train, epoch = 10, batch_size = 64, validation_split = 0.1, verbose = 2)

?fit
?keras::fit
